use derive_builder::Builder;
// This file is @generated by prost-build.
/// API key information.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ApiKey {
    /// A redacted API key. The full API key will not be displayed after it has
    /// been created.
    #[prost(string, tag = "1")]
    pub redacted_api_key: ::prost::alloc::string::String,
    /// ID of the user who created this API key.
    #[prost(string, tag = "3")]
    pub user_id: ::prost::alloc::string::String,
    /// Human-readable name for the API key.
    #[prost(string, tag = "4")]
    pub name: ::prost::alloc::string::String,
    /// Unix timestamp when the API key was created.
    #[prost(message, optional, tag = "5")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Unix timestamp when the API key was last modified.
    #[prost(message, optional, tag = "9")]
    pub modify_time: ::core::option::Option<::prost_types::Timestamp>,
    /// ID of the last user who modified the API key
    #[prost(string, tag = "11")]
    pub modified_by: ::prost::alloc::string::String,
    /// ID of the team this API key belongs to.
    #[prost(string, tag = "6")]
    pub team_id: ::prost::alloc::string::String,
    /// Access Control Lists (ACLs) associated with this key.
    /// These indicate the resources that the API key has access to.
    #[prost(string, repeated, tag = "7")]
    pub acls: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The ID of the API key. This is different from the API key itself.
    #[prost(string, tag = "8")]
    pub api_key_id: ::prost::alloc::string::String,
    /// Whether the API key is currently blocked from making API requests.
    #[prost(bool, tag = "10")]
    pub api_key_blocked: bool,
    /// Whether the team is currently blocked from making API requests.
    #[prost(bool, tag = "13")]
    pub team_blocked: bool,
    /// Whether the API key is currently disabled.
    #[prost(bool, tag = "12")]
    pub disabled: bool,
}
/// Generated client implementations.
pub mod auth_client {
    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// An API service to check status of an API key.
    #[derive(Debug, Clone)]
    pub struct AuthClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl AuthClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> AuthClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> AuthClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + Send + Sync,
        {
            AuthClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Returns some information about an API key.
        pub async fn get_api_key_info(
            &mut self,
            request: impl tonic::IntoRequest<()>,
        ) -> std::result::Result<tonic::Response<super::ApiKey>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Auth/get_api_key_info",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Auth", "get_api_key_info"));
            self.inner.unary(req, path, codec).await
        }
    }
}
/// The response from the service, when creating a deferred completion request.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct StartDeferredResponse {
    /// The ID of this request. This ID can be used to retrieve completion results
    /// later.
    #[prost(string, tag = "1")]
    pub request_id: ::prost::alloc::string::String,
}
/// Retrieve the deferred chat request's response with the `request_id` in
/// StartDeferredResponse.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct GetDeferredRequest {
    /// The ID of this request to get.
    #[prost(string, tag = "1")]
    pub request_id: ::prost::alloc::string::String,
}
/// Status of deferred completion request.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum DeferredStatus {
    /// Invalid status.
    InvalidDeferredStatus = 0,
    /// The request has been processed and is available for download.
    Done = 1,
    /// The request has been processed but the content has expired and is not
    /// available anymore.
    Expired = 2,
    /// The request is still being processed.
    Pending = 3,
}
impl DeferredStatus {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            DeferredStatus::InvalidDeferredStatus => "INVALID_DEFERRED_STATUS",
            DeferredStatus::Done => "DONE",
            DeferredStatus::Expired => "EXPIRED",
            DeferredStatus::Pending => "PENDING",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "INVALID_DEFERRED_STATUS" => Some(Self::InvalidDeferredStatus),
            "DONE" => Some(Self::Done),
            "EXPIRED" => Some(Self::Expired),
            "PENDING" => Some(Self::Pending),
            _ => None,
        }
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct SearchRequest {
    /// The query to search for which will be embedded using the
    /// same embedding model as the one used for the source to query.
    #[prost(string, tag = "1")]
    pub query: ::prost::alloc::string::String,
    /// The source to query.
    #[prost(message, optional, tag = "2")]
    pub source: ::core::option::Option<DocumentsSource>,
    /// The number of chunks to return.
    /// Will always return the top matching chunks.
    /// Optional, defaults to 10
    #[prost(int32, optional, tag = "3")]
    pub limit: ::core::option::Option<i32>,
    /// The ranking metric to use for the search. Defaults to RANK_METRIC_L2_DISTANCE.
    #[prost(enumeration = "RankingMetric", optional, tag = "4")]
    pub ranking_metric: ::core::option::Option<i32>,
    /// User-defined instructions to be included in the search query. Defaults to generic search instructions.
    #[prost(string, optional, tag = "5")]
    pub instructions: ::core::option::Option<::prost::alloc::string::String>,
}
/// SearchResponse message contains the results of a document search operation.
/// It returns a collection of matching document chunks sorted by relevance score.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct SearchResponse {
    /// Collection of document chunks that match the search query, ordered by relevance score
    /// from highest to lowest.
    #[prost(message, repeated, tag = "1")]
    pub matches: ::prost::alloc::vec::Vec<SearchMatch>,
}
/// SearchMatch message represents a single document chunk that matches the search query.
/// It contains the document ID, chunk ID, content text, and a relevance score indicating
/// how well it matches the query.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct SearchMatch {
    /// Unique identifier of the document that contains the matching chunk.
    #[prost(string, tag = "1")]
    pub file_id: ::prost::alloc::string::String,
    /// Unique identifier of the specific chunk within the document that matched the search query.
    #[prost(string, tag = "2")]
    pub chunk_id: ::prost::alloc::string::String,
    /// The actual text content of the matching chunk that can be presented to the user.
    #[prost(string, tag = "3")]
    pub chunk_content: ::prost::alloc::string::String,
    /// Score is the score of the chunk, which is determined by the ranking metric.
    /// For L2 distance, lower scores indicate better matches. Range is [0, inf).
    /// For cosine similarity, higher scores indicate better matches. Range is \[0, 1\].
    #[prost(float, tag = "4")]
    pub score: f32,
    /// The ID(s) of the collection(s) to which this document belongs.
    #[prost(string, repeated, tag = "5")]
    pub collection_ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Configuration for a documents sources in search requests.
///
/// This message configures a source for search content within documents or collections of documents.
/// Those documents must be uploaded through the management API or directly on the xAI console:
/// <https://console.x.ai.>
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct DocumentsSource {
    /// IDs of collections to use.
    #[prost(string, repeated, tag = "1")]
    pub collection_ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// RankingMetric is the metric to use for the search.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum RankingMetric {
    Unknown = 0,
    L2Distance = 1,
    CosineSimilarity = 2,
}
impl RankingMetric {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            RankingMetric::Unknown => "RANKING_METRIC_UNKNOWN",
            RankingMetric::L2Distance => "RANKING_METRIC_L2_DISTANCE",
            RankingMetric::CosineSimilarity => "RANKING_METRIC_COSINE_SIMILARITY",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "RANKING_METRIC_UNKNOWN" => Some(Self::Unknown),
            "RANKING_METRIC_L2_DISTANCE" => Some(Self::L2Distance),
            "RANKING_METRIC_COSINE_SIMILARITY" => Some(Self::CosineSimilarity),
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod documents_client {
    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    #[derive(Debug, Clone)]
    pub struct DocumentsClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl DocumentsClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> DocumentsClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> DocumentsClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + Send + Sync,
        {
            DocumentsClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        pub async fn search(
            &mut self,
            request: impl tonic::IntoRequest<super::SearchRequest>,
        ) -> std::result::Result<tonic::Response<super::SearchResponse>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static("/xai_api.Documents/Search");
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new("xai_api.Documents", "Search"));
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request message for generating an image.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct GenerateImageRequest {
    /// Input prompt to generate an image from.
    #[prost(string, tag = "1")]
    pub prompt: ::prost::alloc::string::String,
    /// Optional input image to perform generations based on.
    #[prost(message, optional, tag = "5")]
    pub image: ::core::option::Option<ImageUrlContent>,
    /// Name or alias of the image generation model to be used.
    #[prost(string, tag = "2")]
    pub model: ::prost::alloc::string::String,
    /// Number of images to generate. Allowed values are \[1, 10\].
    #[prost(int32, optional, tag = "3")]
    pub n: ::core::option::Option<i32>,
    /// An opaque string supplied by the API client (customer) to identify a user.
    /// The string will be stored in the logs and can be used in customer service
    /// requests to identify certain requests.
    #[prost(string, tag = "4")]
    pub user: ::prost::alloc::string::String,
    /// Optional field to specify the image format to return the generated image(s)
    /// in. See ImageFormat enum for options.
    #[prost(enumeration = "ImageFormat", tag = "11")]
    pub format: i32,
}
/// The response from the image generation models containing the generated image(s).
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ImageResponse {
    /// A list of generated images (including relevant metadata).
    #[prost(message, repeated, tag = "1")]
    pub images: ::prost::alloc::vec::Vec<GeneratedImage>,
    /// The model used to generate the image (ignoring aliases).
    #[prost(string, tag = "2")]
    pub model: ::prost::alloc::string::String,
}
/// Contains all data related to a generated image.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct GeneratedImage {
    /// The up sampled prompt that was used to generate the image.
    #[prost(string, tag = "2")]
    pub up_sampled_prompt: ::prost::alloc::string::String,
    /// Whether the image generated by the model respects moderation rules.
    /// The field will be true if the image respect moderation rules. Otherwise
    /// the field will be false and the image field is replaced by a placeholder.
    #[prost(bool, tag = "4")]
    pub respect_moderation: bool,
    /// The generated image.
    #[prost(oneof = "generated_image::Image", tags = "1, 3")]
    pub image: ::core::option::Option<generated_image::Image>,
}
/// Nested message and enum types in `GeneratedImage`.
pub mod generated_image {
    /// The generated image.
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Image {
        /// A base-64 encoded string of the image.
        #[prost(string, tag = "1")]
        Base64(::prost::alloc::string::String),
        /// A url that points to the generated image.
        #[prost(string, tag = "3")]
        Url(::prost::alloc::string::String),
    }
}
/// Contains data relating to an image that is provided to the model.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ImageUrlContent {
    /// This is either an image URL or a base64-encoded version of the image.
    /// The following image formats are supported: PNG and JPG.
    /// If an image URL is provided, the image will be downloaded for every API
    /// request without being cached. Images are fetched using
    /// "XaiImageApiFetch/1.0" user agent, and will timeout after 5 seconds.
    /// The image size is limited to 10 MiB. If the image download fails, the API
    /// request will fail as well.
    #[prost(string, tag = "1")]
    pub image_url: ::prost::alloc::string::String,
    /// The level of pre-processing resolution that will be applied to the image.
    #[prost(enumeration = "ImageDetail", tag = "2")]
    pub detail: i32,
}
/// Indicates the level of preprocessing to apply to images that will be fed to
/// the model.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum ImageDetail {
    /// Detail level is invalid.
    DetailInvalid = 0,
    /// The system will decide the image resolution to use.
    DetailAuto = 1,
    /// The model will process a low-resolution version of the image. This is
    /// faster and cheaper (i.e. consumes fewer tokens).
    DetailLow = 2,
    /// The model will process a high-resolution of the image. This is slower and
    /// more expensive but will allow the model to attend to more nuanced details
    /// in the image.
    DetailHigh = 3,
}
impl ImageDetail {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            ImageDetail::DetailInvalid => "DETAIL_INVALID",
            ImageDetail::DetailAuto => "DETAIL_AUTO",
            ImageDetail::DetailLow => "DETAIL_LOW",
            ImageDetail::DetailHigh => "DETAIL_HIGH",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "DETAIL_INVALID" => Some(Self::DetailInvalid),
            "DETAIL_AUTO" => Some(Self::DetailAuto),
            "DETAIL_LOW" => Some(Self::DetailLow),
            "DETAIL_HIGH" => Some(Self::DetailHigh),
            _ => None,
        }
    }
}
/// The image format to be returned (base-64 encoded string or a url of
/// the image).
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum ImageFormat {
    /// Image format is invalid.
    ImgFormatInvalid = 0,
    /// A base-64 encoding of the image.
    ImgFormatBase64 = 1,
    /// An URL at which the user can download the image.
    ImgFormatUrl = 2,
}
impl ImageFormat {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            ImageFormat::ImgFormatInvalid => "IMG_FORMAT_INVALID",
            ImageFormat::ImgFormatBase64 => "IMG_FORMAT_BASE64",
            ImageFormat::ImgFormatUrl => "IMG_FORMAT_URL",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "IMG_FORMAT_INVALID" => Some(Self::ImgFormatInvalid),
            "IMG_FORMAT_BASE64" => Some(Self::ImgFormatBase64),
            "IMG_FORMAT_URL" => Some(Self::ImgFormatUrl),
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod image_client {
    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// An API service for interaction with image generation models.
    #[derive(Debug, Clone)]
    pub struct ImageClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ImageClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ImageClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ImageClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + Send + Sync,
        {
            ImageClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Create an image based on a text prompt and optionally another image.
        pub async fn generate_image(
            &mut self,
            request: impl tonic::IntoRequest<super::GenerateImageRequest>,
        ) -> std::result::Result<tonic::Response<super::ImageResponse>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Image/GenerateImage",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Image", "GenerateImage"));
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Records the cost associated with a sampling request (both chat and sample
/// endpoints).
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct SamplingUsage {
    /// Total number of text completion tokens generated across all choices
    /// (in case of n>1).
    #[prost(int32, tag = "1")]
    pub completion_tokens: i32,
    /// Total number of reasoning tokens generated across all choices.
    #[prost(int32, tag = "6")]
    pub reasoning_tokens: i32,
    /// Total number of prompt tokens (both text and images).
    #[prost(int32, tag = "2")]
    pub prompt_tokens: i32,
    /// Total number of tokens (prompt + completion).
    #[prost(int32, tag = "3")]
    pub total_tokens: i32,
    /// Total number of (uncached) text tokens in the prompt.
    #[prost(int32, tag = "4")]
    pub prompt_text_tokens: i32,
    /// Total number of cached text tokens in the prompt.
    #[prost(int32, tag = "7")]
    pub cached_prompt_text_tokens: i32,
    /// Total number of image tokens in the prompt.
    #[prost(int32, tag = "5")]
    pub prompt_image_tokens: i32,
    /// Number of individual live search sources used.
    /// Only applicable when live search is enabled.
    /// e.g. If a live search query returns citations from both X and Web and news sources, this will be 3.
    /// If it returns citations from only X, this will be 1.
    #[prost(int32, tag = "8")]
    pub num_sources_used: i32,
}
/// Usage of embedding models.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct EmbeddingUsage {
    /// The number of feature vectors produced from text inputs.
    #[prost(int32, tag = "1")]
    pub num_text_embeddings: i32,
    /// The number of feature vectors produced from image inputs.
    #[prost(int32, tag = "2")]
    pub num_image_embeddings: i32,
}
/// Request to get a text completion response sampling.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct SampleTextRequest {
    /// Text prompts to sample on.
    #[prost(string, repeated, tag = "1")]
    pub prompt: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Name or alias of the model to be used.
    #[prost(string, tag = "3")]
    pub model: ::prost::alloc::string::String,
    /// The number of completions to create concurrently. A single completion will
    /// be generated if the parameter is unset. Each completion is charged at the
    /// same rate. You can generate at most 128 concurrent completions.
    #[prost(int32, optional, tag = "8")]
    pub n: ::core::option::Option<i32>,
    /// The maximum number of tokens to sample. If unset, the model samples until
    /// one of the following stop-conditions is reached:
    /// - The context length of the model is exceeded
    /// - One of the `stop` sequences has been observed.
    ///
    /// We recommend choosing a reasonable value to reduce the risk of accidental
    /// long-generations that consume many tokens.
    #[prost(int32, optional, tag = "7")]
    pub max_tokens: ::core::option::Option<i32>,
    /// A random seed used to make the sampling process deterministic. This is
    /// provided in a best-effort basis without guarantee that sampling is 100%
    /// deterministic given a seed. This is primarily provided for short-lived
    /// testing purposes. Given a fixed request and seed, the answers may change
    /// over time as our systems evolve.
    #[prost(int32, optional, tag = "11")]
    pub seed: ::core::option::Option<i32>,
    /// String patterns that will cause the sampling procedure to stop prematurely
    /// when observed.
    /// Note that the completion is based on individual tokens and sampling can
    /// only terminate at token boundaries. If a stop string is a substring of an
    /// individual token, the completion will include the entire token, which
    /// extends beyond the stop string.
    /// For example, if `stop = \["wor"\]` and we prompt the model with "hello" to
    /// which it responds with "world", then the sampling procedure will stop after
    /// observing the "world" token and the completion will contain
    /// the entire world "world" even though the stop string was just "wor".
    /// You can provide at most 8 stop strings.
    #[prost(string, repeated, tag = "12")]
    pub stop: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// A number between 0 and 2 used to control the variance of completions.
    /// The smaller the value, the more deterministic the model will become. For
    /// example, if we sample 1000 answers to the same prompt at a temperature of
    /// 0.001, then most of the 1000 answers will be identical. Conversely, if we
    /// conduct the same experiment at a temperature of 2, virtually no two answers
    /// will be identical. Note that increasing the temperature will cause
    /// the model to hallucinate more strongly.
    #[prost(float, optional, tag = "14")]
    pub temperature: ::core::option::Option<f32>,
    /// A number between 0 and 1 controlling the likelihood of the model to use
    /// less-common answers. Recall that the model produces a probability for
    /// each token. This means, for any choice of token there are thousands of
    /// possibilities to choose from. This parameter controls the "nucleus sampling
    /// algorithm". Instead of considering every possible token at every step, we
    /// only look at the K tokens who's probabilities exceed `top_p`.
    /// For example, if we set `top_p = 0.9`, then the set of tokens we actually
    /// sample from, will have a probability mass of at least 90%. In practice,
    /// low values will make the model more deterministic.
    #[prost(float, optional, tag = "15")]
    pub top_p: ::core::option::Option<f32>,
    /// Number between -2.0 and 2.0.
    /// Positive values penalize new tokens based on their existing frequency in the text so far,
    /// decreasing the model's likelihood to repeat the same line verbatim.
    #[prost(float, optional, tag = "13")]
    pub frequency_penalty: ::core::option::Option<f32>,
    /// Whether to return log probabilities of the output tokens or not.
    /// If true, returns the log probabilities of each output token returned in the content of message.
    #[prost(bool, tag = "5")]
    pub logprobs: bool,
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    /// Not supported by grok-3 models.
    #[prost(float, optional, tag = "9")]
    pub presence_penalty: ::core::option::Option<f32>,
    /// An integer between 0 and 8 specifying the number of most likely tokens to return at each token position,
    /// each with an associated log probability.
    /// logprobs must be set to true if this parameter is used.
    #[prost(int32, optional, tag = "6")]
    pub top_logprobs: ::core::option::Option<i32>,
    /// An opaque string supplied by the API client (customer) to identify a user.
    /// The string will be stored in the logs and can be used in customer service
    /// requests to identify certain requests.
    #[prost(string, tag = "17")]
    pub user: ::prost::alloc::string::String,
}
/// Response of a text completion response sampling.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct SampleTextResponse {
    /// The ID of this request. This ID will also show up on your billing records
    /// and you can use it when contacting us regarding a specific request.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// Completions in response to the input messages. The number of completions is
    /// controlled via the `n` parameter on the request.
    #[prost(message, repeated, tag = "2")]
    pub choices: ::prost::alloc::vec::Vec<SampleChoice>,
    /// A UNIX timestamp (UTC) indicating when the response object was created.
    /// The timestamp is taken when the model starts generating response.
    #[prost(message, optional, tag = "5")]
    pub created: ::core::option::Option<::prost_types::Timestamp>,
    /// The name of the model used for the request. This model name contains
    /// the actual model name used rather than any aliases.
    /// This means the this can be `grok-2-1212` even when the request was
    /// specifying `grok-2-latest`.
    #[prost(string, tag = "6")]
    pub model: ::prost::alloc::string::String,
    /// Note supported yet. Included for compatibility reasons.
    #[prost(string, tag = "7")]
    pub system_fingerprint: ::prost::alloc::string::String,
    /// The number of tokens consumed by this request.
    #[prost(message, optional, tag = "9")]
    pub usage: ::core::option::Option<SamplingUsage>,
}
/// Contains the response generated by the model.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct SampleChoice {
    /// Indicating why the model stopped sampling.
    #[prost(enumeration = "FinishReason", tag = "1")]
    pub finish_reason: i32,
    /// The index of this choice in the list of choices. If you set `n > 1` on
    /// your request, you will receive more than one choice in your response.
    #[prost(int32, tag = "2")]
    pub index: i32,
    /// The actual text generated by the model.
    #[prost(string, tag = "3")]
    pub text: ::prost::alloc::string::String,
}
/// Reasons why the model stopped sampling.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum FinishReason {
    /// Invalid reason.
    ReasonInvalid = 0,
    /// The max_len parameter specified on the input is reached.
    ReasonMaxLen = 1,
    /// The maximum context length of the model is reached.
    ReasonMaxContext = 2,
    /// One of the stop words was found.
    ReasonStop = 3,
    /// A tool call is included in the response.
    ReasonToolCalls = 4,
    /// Time limit has been reached.
    ReasonTimeLimit = 5,
}
impl FinishReason {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            FinishReason::ReasonInvalid => "REASON_INVALID",
            FinishReason::ReasonMaxLen => "REASON_MAX_LEN",
            FinishReason::ReasonMaxContext => "REASON_MAX_CONTEXT",
            FinishReason::ReasonStop => "REASON_STOP",
            FinishReason::ReasonToolCalls => "REASON_TOOL_CALLS",
            FinishReason::ReasonTimeLimit => "REASON_TIME_LIMIT",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "REASON_INVALID" => Some(Self::ReasonInvalid),
            "REASON_MAX_LEN" => Some(Self::ReasonMaxLen),
            "REASON_MAX_CONTEXT" => Some(Self::ReasonMaxContext),
            "REASON_STOP" => Some(Self::ReasonStop),
            "REASON_TOOL_CALLS" => Some(Self::ReasonToolCalls),
            "REASON_TIME_LIMIT" => Some(Self::ReasonTimeLimit),
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod sample_client {
    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// An API service for sampling the responses of available language models.
    #[derive(Debug, Clone)]
    pub struct SampleClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl SampleClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> SampleClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> SampleClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + Send + Sync,
        {
            SampleClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Get raw sampling of text response from the model inference.
        pub async fn sample_text(
            &mut self,
            request: impl tonic::IntoRequest<super::SampleTextRequest>,
        ) -> std::result::Result<
            tonic::Response<super::SampleTextResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Sample/SampleText",
            );
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new("xai_api.Sample", "SampleText"));
            self.inner.unary(req, path, codec).await
        }
        /// Get streaming raw sampling of text response from the model inference.
        pub async fn sample_text_streaming(
            &mut self,
            request: impl tonic::IntoRequest<super::SampleTextRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::SampleTextResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Sample/SampleTextStreaming",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Sample", "SampleTextStreaming"));
            self.inner.server_streaming(req, path, codec).await
        }
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct GetCompletionsRequest {
    /// A sequence of messages in the conversation. There must be at least a single
    /// message that the model can respond to.
    #[prost(message, repeated, tag = "1")]
    pub messages: ::prost::alloc::vec::Vec<Message>,
    /// Name of the model. This is the name as reported by the models API. More
    /// details can be found on your console at <https://console.x.ai.>
    #[prost(string, tag = "2")]
    pub model: ::prost::alloc::string::String,
    /// An opaque string supplied by the API client (customer) to identify a user.
    /// The string will be stored in the logs and can be used in customer service
    /// requests to identify certain requests.
    #[prost(string, tag = "16")]
    pub user: ::prost::alloc::string::String,
    /// The number of completions to create concurrently. A single completion will
    /// be generated if the parameter is unset. Each completion is charged at the
    /// same rate. You can generate at most 128 concurrent completions.
    #[prost(int32, optional, tag = "8")]
    pub n: ::core::option::Option<i32>,
    /// The maximum number of tokens to sample. If unset, the model samples until
    /// one of the following stop-conditions is reached:
    /// - The context length of the model is exceeded
    /// - One of the `stop` sequences has been observed.
    /// - The time limit exceeds.
    ///
    /// Note that for reasoning models and models that support function calls, the
    /// limit is only applied to the main content and not to the reasoning content
    /// or function calls.
    ///
    /// We recommend choosing a reasonable value to reduce the risk of accidental
    /// long-generations that consume many tokens.
    #[prost(int32, optional, tag = "7")]
    pub max_tokens: ::core::option::Option<i32>,
    /// A random seed used to make the sampling process deterministic. This is
    /// provided in a best-effort basis without guarantee that sampling is 100%
    /// deterministic given a seed. This is primarily provided for short-lived
    /// testing purposes. Given a fixed request and seed, the answers may change
    /// over time as our systems evolve.
    #[prost(int32, optional, tag = "11")]
    pub seed: ::core::option::Option<i32>,
    /// String patterns that will cause the sampling procedure to stop prematurely
    /// when observed.
    /// Note that the completion is based on individual tokens and sampling can
    /// only terminate at token boundaries. If a stop string is a substring of an
    /// individual token, the completion will include the entire token, which
    /// extends beyond the stop string.
    /// For example, if `stop = \["wor"\]` and we prompt the model with "hello" to
    /// which it responds with "world", then the sampling procedure will stop after
    /// observing the "world" token and the completion will contain
    /// the entire world "world" even though the stop string was just "wor".
    /// You can provide at most 8 stop strings.
    #[prost(string, repeated, tag = "12")]
    pub stop: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// A number between 0 and 2 used to control the variance of completions.
    /// The smaller the value, the more deterministic the model will become. For
    /// example, if we sample 1000 answers to the same prompt at a temperature of
    /// 0.001, then most of the 1000 answers will be identical. Conversely, if we
    /// conduct the same experiment at a temperature of 2, virtually no two answers
    /// will be identical. Note that increasing the temperature will cause
    /// the model to hallucinate more strongly.
    #[prost(float, optional, tag = "14")]
    pub temperature: ::core::option::Option<f32>,
    /// A number between 0 and 1 controlling the likelihood of the model to use
    /// less-common answers. Recall that the model produces a probability for
    /// each token. This means, for any choice of token there are thousands of
    /// possibilities to choose from. This parameter controls the "nucleus sampling
    /// algorithm". Instead of considering every possible token at every step, we
    /// only look at the K tokens who's probabilities exceed `top_p`.
    /// For example, if we set `top_p = 0.9`, then the set of tokens we actually
    /// sample from, will have a probability mass of at least 90%. In practice,
    /// low values will make the model more deterministic.
    #[prost(float, optional, tag = "15")]
    pub top_p: ::core::option::Option<f32>,
    /// If set to true, log probabilities of the sampling are returned.
    #[prost(bool, tag = "5")]
    pub logprobs: bool,
    /// Number of top log probabilities to return.
    #[prost(int32, optional, tag = "6")]
    pub top_logprobs: ::core::option::Option<i32>,
    /// A list of tools the model may call. Currently, only functions are supported
    /// as a tool. Use this to provide a list of functions the model may generate
    /// JSON inputs for.
    #[prost(message, repeated, tag = "17")]
    pub tools: ::prost::alloc::vec::Vec<Tool>,
    /// Controls if the model can, should, or must not use tools.
    #[prost(message, optional, tag = "18")]
    pub tool_choice: ::core::option::Option<ToolChoice>,
    /// Formatting constraint on the response.
    #[prost(message, optional, tag = "10")]
    pub response_format: ::core::option::Option<ResponseFormat>,
    /// Positive values penalize new tokens based on their existing frequency in
    /// the text so far, decreasing the model's likelihood to repeat the same line
    /// verbatim.
    #[prost(float, optional, tag = "3")]
    pub frequency_penalty: ::core::option::Option<f32>,
    /// Positive values penalize new tokens based on whether they appear in
    /// the text so far, increasing the model's likelihood to talk about
    /// new topics.
    #[prost(float, optional, tag = "9")]
    pub presence_penalty: ::core::option::Option<f32>,
    /// Constrains effort on reasoning for reasoning models. Default to `EFFORT_MEDIUM`.
    #[prost(enumeration = "ReasoningEffort", optional, tag = "19")]
    pub reasoning_effort: ::core::option::Option<i32>,
    /// Set the parameters to be used for realtime data. If not set, no realtime data will be acquired by the model.
    #[prost(message, optional, tag = "20")]
    pub search_parameters: ::core::option::Option<SearchParameters>,
    /// / If set to false, the model can perform maximum one tool call per response. Default to true.
    #[prost(bool, optional, tag = "21")]
    pub parallel_tool_calls: ::core::option::Option<bool>,
    /// Previous response id. The messages from this response must be chained.
    #[prost(string, optional, tag = "22")]
    pub previous_response_id: ::core::option::Option<::prost::alloc::string::String>,
    /// Whether to store request and responses. Default is false.
    #[prost(bool, tag = "23")]
    pub store_messages: bool,
    /// Whether to use encrypted thinking for thinking trace rehydration.
    #[prost(bool, tag = "24")]
    pub use_encrypted_content: bool,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct GetChatCompletionResponse {
    /// The ID of this request. This ID will also show up on your billing records
    /// and you can use it when contacting us regarding a specific request.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// Completions in response to the input messages. The number of completions is
    /// controlled via the `n` parameter on the request.
    #[prost(message, repeated, tag = "2")]
    pub choices: ::prost::alloc::vec::Vec<Choice>,
    /// A UNIX timestamp (UTC) indicating when the response object was created.
    /// The timestamp is taken when the model starts generating response.
    #[prost(message, optional, tag = "5")]
    pub created: ::core::option::Option<::prost_types::Timestamp>,
    /// The name of the model used for the request. This model name contains
    /// the actual model name used rather than any aliases.
    /// This means the this can be `grok-2-1212` even when the request was
    /// specifying `grok-2-latest`.
    #[prost(string, tag = "6")]
    pub model: ::prost::alloc::string::String,
    /// This fingerprint represents the backend configuration that the model runs
    /// with.
    #[prost(string, tag = "7")]
    pub system_fingerprint: ::prost::alloc::string::String,
    /// The number of tokens consumed by this request.
    #[prost(message, optional, tag = "9")]
    pub usage: ::core::option::Option<SamplingUsage>,
    /// / List of all the external pages (urls) used by the model to produce its final answer.
    /// This is only present when live search is enabled, (That is `SearchParameters` have been defined in `GetCompletionsRequest`).
    #[prost(string, repeated, tag = "10")]
    pub citations: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Settings used while generating the response.
    #[prost(message, optional, tag = "11")]
    pub settings: ::core::option::Option<RequestSettings>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct GetChatCompletionChunk {
    /// The ID of this request. This ID will also show up on your billing records
    /// and you can use it when contacting us regarding a specific request.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// The choices of the model.
    #[prost(message, repeated, tag = "2")]
    pub choices: ::prost::alloc::vec::Vec<ChoiceChunk>,
    /// A UNIX timestamp (UTC) indicating when the response object was created.
    /// The timestamp is taken when the model starts generating response.
    #[prost(message, optional, tag = "3")]
    pub created: ::core::option::Option<::prost_types::Timestamp>,
    /// The name of the model used for the request. This model name contains
    /// the actual model name used rather than any aliases.
    /// This means the this can be `grok-2-1212` even when the request was
    /// specifying `grok-2-latest`.
    #[prost(string, tag = "4")]
    pub model: ::prost::alloc::string::String,
    /// This fingerprint represents the backend configuration that the model runs
    /// with.
    #[prost(string, tag = "5")]
    pub system_fingerprint: ::prost::alloc::string::String,
    /// The total number of tokens consumed when this chunk was streamed. Note that
    /// this is not the final number of tokens billed unless this is the last chunk
    /// in the stream.
    #[prost(message, optional, tag = "6")]
    pub usage: ::core::option::Option<SamplingUsage>,
    /// / List of all the external pages used by the model to answer. Only populated for the last chunk.
    /// This is only present when live search is enabled, (That is `SearchParameters` have been defined in `GetCompletionsRequest`).
    #[prost(string, repeated, tag = "7")]
    pub citations: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Response from GetDeferredCompletion, including the response if the completion
/// request has been processed without error.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct GetDeferredCompletionResponse {
    /// Current status of the request.
    #[prost(enumeration = "DeferredStatus", tag = "2")]
    pub status: i32,
    /// Response. Only present if `status=DONE`
    #[prost(message, optional, tag = "1")]
    pub response: ::core::option::Option<GetChatCompletionResponse>,
}
/// Contains the response generated by the model.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct Choice {
    /// Indicating why the model stopped sampling.
    #[prost(enumeration = "FinishReason", tag = "1")]
    pub finish_reason: i32,
    /// The index of this choice in the list of choices. If you set `n > 1` on your
    /// request, you will receive most than one choice in your response.
    #[prost(int32, tag = "2")]
    pub index: i32,
    /// The actual message generated by the model.
    #[prost(message, optional, tag = "3")]
    pub message: ::core::option::Option<CompletionMessage>,
    /// The log probabilities of the sampling.
    #[prost(message, optional, tag = "4")]
    pub logprobs: ::core::option::Option<LogProbs>,
}
/// Holds the model output (i.e. the result of the sampling process).
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct CompletionMessage {
    /// The generated text based on the input prompt.
    #[prost(string, tag = "1")]
    pub content: ::prost::alloc::string::String,
    /// Reasoning trace the model produced before issuing the final answer.
    #[prost(string, tag = "4")]
    pub reasoning_content: ::prost::alloc::string::String,
    /// The role of the message author. Will always default to "assistant".
    #[prost(enumeration = "MessageRole", tag = "2")]
    pub role: i32,
    /// The tools that the assistant wants to call.
    #[prost(message, repeated, tag = "3")]
    pub tool_calls: ::prost::alloc::vec::Vec<ToolCall>,
    /// The encrypted thinking content.
    #[prost(string, tag = "5")]
    pub encrypted_content: ::prost::alloc::string::String,
}
/// Holds the differences (deltas) that when concatenated make up the entire
/// agent response.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ChoiceChunk {
    /// The actual text differences that need to be accumulated on the client.
    #[prost(message, optional, tag = "1")]
    pub delta: ::core::option::Option<Delta>,
    /// The log probability of the choice.
    #[prost(message, optional, tag = "2")]
    pub logprobs: ::core::option::Option<LogProbs>,
    /// Indicating why the model stopped sampling.
    #[prost(enumeration = "FinishReason", tag = "3")]
    pub finish_reason: i32,
    /// The index of this choice in the list of choices. If you set `n > 1` on your
    /// request, you will receive most than one choice in your response.
    #[prost(int32, tag = "4")]
    pub index: i32,
}
/// The delta of a streaming response.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct Delta {
    /// The main model output/answer.
    #[prost(string, tag = "1")]
    pub content: ::prost::alloc::string::String,
    /// Part of the model's reasoning trace.
    #[prost(string, tag = "4")]
    pub reasoning_content: ::prost::alloc::string::String,
    /// The entity type who sent the message. For example, a message can be sent by
    /// a user or the assistant.
    #[prost(enumeration = "MessageRole", tag = "2")]
    pub role: i32,
    /// A list of tool calls if tool call is requested by the model.
    #[prost(message, repeated, tag = "3")]
    pub tool_calls: ::prost::alloc::vec::Vec<ToolCall>,
    /// The encrypted thinking content.
    #[prost(string, tag = "5")]
    pub encrypted_content: ::prost::alloc::string::String,
}
/// Holding the log probabilities of the sampling.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct LogProbs {
    /// A list of log probability entries, each corresponding to a sampled token
    /// and its associated data.
    #[prost(message, repeated, tag = "1")]
    pub content: ::prost::alloc::vec::Vec<LogProb>,
}
/// Represents the logarithmic probability and metadata for a single sampled
/// token.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct LogProb {
    /// The text representation of the sampled token.
    #[prost(string, tag = "1")]
    pub token: ::prost::alloc::string::String,
    /// The logarithmic probability of this token being sampled, given the prior
    /// context.
    #[prost(float, tag = "2")]
    pub logprob: f32,
    /// The raw byte representation of the token, useful for handling non-text or
    /// encoded data.
    #[prost(bytes = "vec", tag = "3")]
    pub bytes: ::prost::alloc::vec::Vec<u8>,
    /// A list of the top alternative tokens and their log probabilities at this
    /// sampling step.
    #[prost(message, repeated, tag = "4")]
    pub top_logprobs: ::prost::alloc::vec::Vec<TopLogProb>,
}
/// Represents an alternative token and its log probability among the top
/// candidates.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct TopLogProb {
    /// The text representation of an alternative token considered by the model.
    #[prost(string, tag = "1")]
    pub token: ::prost::alloc::string::String,
    /// The logarithmic probability of this alternative token being sampled.
    #[prost(float, tag = "2")]
    pub logprob: f32,
    /// The raw byte representation of the alternative token.
    #[prost(bytes = "vec", tag = "3")]
    pub bytes: ::prost::alloc::vec::Vec<u8>,
}
/// Holds a single content element that is part of an input message.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct Content {
    #[prost(oneof = "content::Content", tags = "1, 2")]
    pub content: ::core::option::Option<content::Content>,
}
/// Nested message and enum types in `Content`.
pub mod content {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Content {
        /// The content is a pure text message.
        #[prost(string, tag = "1")]
        Text(::prost::alloc::string::String),
        /// The content is a single image.
        #[prost(message, tag = "2")]
        ImageUrl(super::ImageUrlContent),
    }
}
/// A message in a conversation. This message is part of the model input. Each
/// message originates from a "role", which indicates the entity type who sent
/// the message. Messages can contain multiple content elements such as text and
/// images.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct Message {
    /// The content of the message. Some model support multi-modal message contents
    /// that consist of text and images. At least one content element must be set
    /// for each message.
    #[prost(message, repeated, tag = "1")]
    pub content: ::prost::alloc::vec::Vec<Content>,
    /// Reasoning trace the model produced before issuing the final answer.
    #[prost(string, optional, tag = "5")]
    pub reasoning_content: ::core::option::Option<::prost::alloc::string::String>,
    /// The entity type who sent the message. For example, a message can be sent by
    /// a user or the assistant.
    #[prost(enumeration = "MessageRole", tag = "2")]
    pub role: i32,
    /// The name of the entity who sent the message. The name can only be set if
    /// the role is ROLE_USER.
    #[prost(string, tag = "3")]
    pub name: ::prost::alloc::string::String,
    /// The tools that the assistant wants to call.
    #[prost(message, repeated, tag = "4")]
    pub tool_calls: ::prost::alloc::vec::Vec<ToolCall>,
    /// The encrypted thinking content.
    #[prost(string, tag = "6")]
    pub encrypted_content: ::prost::alloc::string::String,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ToolChoice {
    #[prost(oneof = "tool_choice::ToolChoice", tags = "1, 2")]
    pub tool_choice: ::core::option::Option<tool_choice::ToolChoice>,
}
/// Nested message and enum types in `ToolChoice`.
pub mod tool_choice {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum ToolChoice {
        /// Force the model to perform in a given mode.
        #[prost(enumeration = "super::ToolMode", tag = "1")]
        Mode(i32),
        /// Force the model to call a particular function.
        #[prost(string, tag = "2")]
        FunctionName(::prost::alloc::string::String),
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct Tool {
    #[prost(oneof = "tool::Tool", tags = "1, 3, 4")]
    pub tool: ::core::option::Option<tool::Tool>,
}
/// Nested message and enum types in `Tool`.
pub mod tool {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Tool {
        /// Tool Call defined by user
        #[prost(message, tag = "1")]
        Function(super::Function),
        /// Built in web search.
        #[prost(message, tag = "3")]
        WebSearch(super::WebSearch),
        /// Built in X search.
        #[prost(message, tag = "4")]
        XSearch(super::XSearch),
    }
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct WebSearch {}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct XSearch {}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct Function {
    /// Name of the function.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Description of the function.
    #[prost(string, tag = "2")]
    pub description: ::prost::alloc::string::String,
    /// Not supported: Only kept for compatibility reasons.
    #[prost(bool, tag = "3")]
    pub strict: bool,
    /// The parameters the functions accepts, described as a JSON Schema object.
    #[prost(string, tag = "4")]
    pub parameters: ::prost::alloc::string::String,
}
/// Content of a tool call, typically in a response from model.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ToolCall {
    /// The ID of the tool call.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// Information regarding invoking the tool call.
    #[prost(oneof = "tool_call::Tool", tags = "10")]
    pub tool: ::core::option::Option<tool_call::Tool>,
}
/// Nested message and enum types in `ToolCall`.
pub mod tool_call {
    /// Information regarding invoking the tool call.
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Tool {
        #[prost(message, tag = "10")]
        Function(super::FunctionCall),
    }
}
/// Tool call information.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct FunctionCall {
    /// Name of the function to call.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Arguments used to call the function as json string.
    #[prost(string, tag = "2")]
    pub arguments: ::prost::alloc::string::String,
}
/// The response format for structured response.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ResponseFormat {
    /// Type of format expected for the response. Default to `FORMAT_TYPE_TEXT`
    #[prost(enumeration = "FormatType", tag = "1")]
    pub format_type: i32,
    /// The JSON schema that the response should conform to.
    /// Only considered if `format_type` is `FORMAT_TYPE_JSON_SCHEMA`.
    #[prost(string, optional, tag = "2")]
    pub schema: ::core::option::Option<::prost::alloc::string::String>,
}
/// Parameters for configuring search behavior in a chat request.
///
/// This message allows customization of search functionality when using models that support
/// searching external sources for information. You can specify which sources to search,
/// set date ranges for relevant content, control the search mode, and configure how
/// results are returned.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct SearchParameters {
    /// Controls when search is performed. Possible values are:
    ///    - OFF_SEARCH_MODE (default): No search is performed, and no external data will be considered.
    ///    - ON_SEARCH_MODE: Search is always performed when sampling from the model and the model will search in every source provided for relevant data.
    ///    - AUTO_SEARCH_MODE: The model decides whether to perform a search based on the prompt and which sources to use.
    #[prost(enumeration = "SearchMode", tag = "1")]
    pub mode: i32,
    /// A list of search sources to query, such as web, news, X, or RSS feeds.
    /// Multiple sources can be specified. If no sources are provided, the model will default to
    /// searching the web and X.
    #[prost(message, repeated, tag = "9")]
    pub sources: ::prost::alloc::vec::Vec<Source>,
    /// Optional start date for search results in ISO-8601 YYYY-MM-DD format (e.g., "2024-05-24").
    /// Only content after this date will be considered. Defaults to unset (no start date restriction).
    /// See <https://en.wikipedia.org/wiki/ISO_8601> for format details.
    #[prost(message, optional, tag = "4")]
    pub from_date: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional end date for search results in ISO-8601 YYYY-MM-DD format (e.g., "2024-12-24").
    /// Only content before this date will be considered. Defaults to unset (no end date restriction).
    /// See <https://en.wikipedia.org/wiki/ISO_8601> for format details.
    #[prost(message, optional, tag = "5")]
    pub to_date: ::core::option::Option<::prost_types::Timestamp>,
    /// If set to true, the model will return a list of citations (URLs or references)
    /// to the sources used in generating the response. Defaults to true.
    #[prost(bool, tag = "7")]
    pub return_citations: bool,
    /// Optional limit on the number of search results to consider
    /// when generating a response. Must be in the range \[1, 30\]. Defaults to 15.
    #[prost(int32, optional, tag = "8")]
    pub max_search_results: ::core::option::Option<i32>,
}
/// Defines a source for search requests, specifying the type of content to search.
/// This message acts as a container for different types of search sources. Only one type
/// of source can be specified per instance using the oneof field.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct Source {
    #[prost(oneof = "source::Source", tags = "1, 2, 3, 4")]
    pub source: ::core::option::Option<source::Source>,
}
/// Nested message and enum types in `Source`.
pub mod source {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Source {
        /// Configuration for searching online web content. Use this to search general websites
        /// with options to filter by country, exclude specific domains, or only allow specific domains.
        #[prost(message, tag = "1")]
        Web(super::WebSource),
        /// Configuration for searching recent articles and reports from news outlets.
        /// Useful for current events or topic-specific updates.
        #[prost(message, tag = "2")]
        News(super::NewsSource),
        /// Configuration for searching content on X. Allows focusing on
        /// specific user handles for targeted content.
        #[prost(message, tag = "3")]
        X(super::XSource),
        /// Configuration for searching content from RSS feeds. Requires specific feed URLs
        /// to query.
        #[prost(message, tag = "4")]
        Rss(super::RssSource),
    }
}
/// Configuration for a web search source in search requests.
///
/// This message configures a source for searching online web content. It allows specification
/// of regional content through country codes and filtering of results by excluding or allowing
/// specific websites.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct WebSource {
    /// List of website domains (without protocol specification or subdomains) to exclude from search results (e.g., \["example.com"\]).
    /// Use this to prevent results from unwanted sites. A maximum of 5 websites can be excluded.
    /// This parameter cannot be set together with `allowed_websites`.
    #[prost(string, repeated, tag = "2")]
    pub excluded_websites: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// List of website domains (without protocol specification or subdomains)
    /// to restrict search results to (e.g., \["example.com"\]). A maximum of 5 websites can be allowed.
    /// Use this as a whitelist to limit results to only these specific sites; no other websites will
    /// be considered. If no relevant information is found on these websites, the number of results
    /// returned might be smaller than `max_search_results` set in `SearchParameters`. Note: This
    /// parameter cannot be set together with `excluded_websites`.
    #[prost(string, repeated, tag = "5")]
    pub allowed_websites: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional ISO alpha-2 country code (e.g., "BE" for Belgium) to limit search results
    /// to content from a specific region or country. Defaults to unset (global search).
    /// See <https://en.wikipedia.org/wiki/ISO_3166-2> for valid codes.
    #[prost(string, optional, tag = "3")]
    pub country: ::core::option::Option<::prost::alloc::string::String>,
    /// Whether to exclude adult content from the search results. Defaults to true.
    #[prost(bool, tag = "4")]
    pub safe_search: bool,
}
/// Configuration for a news search source in search requests.
///
/// This message configures a source for searching recent articles and reports from news outlets.
/// It is useful for obtaining current events or topic-specific updates with regional filtering.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct NewsSource {
    /// List of website domains (without protocol specification or subdomains)
    /// to exclude from search results (e.g., \["example.com"\]). A maximum of 5 websites can be excluded.
    /// Use this to prevent results from specific news sites. Defaults to unset (no exclusions).
    #[prost(string, repeated, tag = "2")]
    pub excluded_websites: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional ISO alpha-2 country code (e.g., "BE" for Belgium) to limit search results
    /// to news from a specific region or country. Defaults to unset (global news).
    /// See <https://en.wikipedia.org/wiki/ISO_3166-2> for valid codes.
    #[prost(string, optional, tag = "3")]
    pub country: ::core::option::Option<::prost::alloc::string::String>,
    /// Whether to exclude adult content from the search results. Defaults to true.
    #[prost(bool, tag = "4")]
    pub safe_search: bool,
}
/// Configuration for an X (formerly Twitter) search source in search requests.
///
/// This message configures a source for searching content on X. It allows focusing the search
/// on specific user handles to retrieve targeted posts and interactions.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct XSource {
    /// Optional list of X usernames (without the '@' symbol) to limit search results to posts
    /// from specific accounts (e.g., \["xai"\]). If set, only posts authored by these
    /// handles will be considered in the live search.
    /// This field can not be set together with `excluded_x_handles`.
    /// Defaults to unset (no exclusions).
    #[prost(string, repeated, tag = "7")]
    pub included_x_handles: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional list of X usernames (without the '@' symbol) used to exclude posts from specific accounts.
    /// If set, posts authored by these handles will be excluded from the live search results.
    /// This field can not be set together with `included_x_handles`.
    /// Defaults to unset (no exclusions).
    #[prost(string, repeated, tag = "8")]
    pub excluded_x_handles: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional post favorite count threshold. Defaults to unset (don't filter posts by post favorite count).
    /// If set, only posts with a favorite count greater than or equal to this threshold will be considered.
    #[prost(int32, optional, tag = "9")]
    pub post_favorite_count: ::core::option::Option<i32>,
    /// Optional post view count threshold. Defaults to unset (don't filter posts by post view count).
    /// If set, only posts with a view count greater than or equal to this threshold will be considered.
    #[prost(int32, optional, tag = "10")]
    pub post_view_count: ::core::option::Option<i32>,
}
/// Configuration for an RSS search source in search requests.
///
/// This message configures a source for searching content from RSS feeds. It requires specific
/// feed URLs to query for content updates.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct RssSource {
    /// List of RSS feed URLs to search. Each URL must point to a valid RSS feed.
    /// At least one link must be provided.
    #[prost(string, repeated, tag = "1")]
    pub links: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct RequestSettings {
    /// Max number of tokens that can be generated in a response. This includes both output and reasoning tokens.
    #[prost(int32, optional, tag = "1")]
    pub max_tokens: ::core::option::Option<i32>,
    /// / If set to false, the model can perform maximum one tool call. Default to true.
    #[prost(bool, tag = "2")]
    pub parallel_tool_calls: bool,
    /// The ID of the previous response from the model.
    #[prost(string, optional, tag = "3")]
    pub previous_response_id: ::core::option::Option<::prost::alloc::string::String>,
    /// Constrains effort on reasoning for reasoning models. Default to `EFFORT_MEDIUM`.
    #[prost(enumeration = "ReasoningEffort", optional, tag = "4")]
    pub reasoning_effort: ::core::option::Option<i32>,
    /// A number between 0 and 2 used to control the variance of completions.
    /// The smaller the value, the more deterministic the model will become. For
    /// example, if we sample 1000 answers to the same prompt at a temperature of
    /// 0.001, then most of the 1000 answers will be identical. Conversely, if we
    /// conduct the same experiment at a temperature of 2, virtually no two answers
    /// will be identical. Note that increasing the temperature will cause
    /// the model to hallucinate more strongly.
    #[prost(float, optional, tag = "5")]
    pub temperature: ::core::option::Option<f32>,
    /// Formatting constraint on the response.
    #[prost(message, optional, tag = "6")]
    pub response_format: ::core::option::Option<ResponseFormat>,
    /// Controls if the model can, should, or must not use tools.
    #[prost(message, optional, tag = "7")]
    pub tool_choice: ::core::option::Option<ToolChoice>,
    /// A list of tools the model may call. Currently, only functions are supported
    /// as a tool. Use this to provide a list of functions the model may generate
    /// JSON inputs for.
    #[prost(message, repeated, tag = "8")]
    pub tools: ::prost::alloc::vec::Vec<Tool>,
    /// A number between 0 and 1 controlling the likelihood of the model to use
    /// less-common answers. Recall that the model produces a probability for
    /// each token. This means, for any choice of token there are thousands of
    /// possibilities to choose from. This parameter controls the "nucleus sampling
    /// algorithm". Instead of considering every possible token at every step, we
    /// only look at the K tokens who's probabilities exceed `top_p`.
    /// For example, if we set `top_p = 0.9`, then the set of tokens we actually
    /// sample from, will have a probability mass of at least 90%. In practice,
    /// low values will make the model more deterministic.
    #[prost(float, optional, tag = "9")]
    pub top_p: ::core::option::Option<f32>,
    /// An opaque string supplied by the API client (customer) to identify a user.
    /// The string will be stored in the logs and can be used in customer service
    /// requests to identify certain requests.
    #[prost(string, tag = "10")]
    pub user: ::prost::alloc::string::String,
    /// Set the parameters to be used for realtime data. If not set, no realtime data will be acquired by the model.
    #[prost(message, optional, tag = "11")]
    pub search_parameters: ::core::option::Option<SearchParameters>,
    /// Whether to store request and responses. Default is false.
    #[prost(bool, tag = "12")]
    pub store_messages: bool,
    /// Whether to use encrypted thinking for thinking trace rehydration.
    #[prost(bool, tag = "13")]
    pub use_encrypted_content: bool,
}
/// History of a user's messages.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ResponseHistory {
    /// The history of messages uptil (and including) the current response.
    #[prost(message, repeated, tag = "1")]
    pub messages: ::prost::alloc::vec::Vec<Message>,
    /// The actual response.
    #[prost(message, optional, tag = "2")]
    pub response: ::core::option::Option<GetChatCompletionResponse>,
}
/// Request to retrieve a stored completion response.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct GetStoredCompletionRequest {
    /// The response id to be retrieved.
    #[prost(string, tag = "1")]
    pub response_id: ::prost::alloc::string::String,
}
/// Request to delete a stored completion response.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct DeleteStoredCompletionRequest {
    /// The response id to be deleted.
    #[prost(string, tag = "1")]
    pub response_id: ::prost::alloc::string::String,
}
/// Response for deleting a stored completion.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct DeleteStoredCompletionResponse {
    /// The response id that was deleted.
    #[prost(string, tag = "1")]
    pub response_id: ::prost::alloc::string::String,
}
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum MessageRole {
    /// Default value / invalid role.
    InvalidRole = 0,
    /// User role.
    RoleUser = 1,
    /// Assistant role, normally the response from the model.
    RoleAssistant = 2,
    /// System role, typically for system instructions.
    RoleSystem = 3,
    /// Indicates a return from a tool call. Deprecated in favor of ROLE_TOOL.
    RoleFunction = 4,
    /// Indicates a return from a tool call.
    RoleTool = 5,
}
impl MessageRole {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            MessageRole::InvalidRole => "INVALID_ROLE",
            MessageRole::RoleUser => "ROLE_USER",
            MessageRole::RoleAssistant => "ROLE_ASSISTANT",
            MessageRole::RoleSystem => "ROLE_SYSTEM",
            MessageRole::RoleFunction => "ROLE_FUNCTION",
            MessageRole::RoleTool => "ROLE_TOOL",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "INVALID_ROLE" => Some(Self::InvalidRole),
            "ROLE_USER" => Some(Self::RoleUser),
            "ROLE_ASSISTANT" => Some(Self::RoleAssistant),
            "ROLE_SYSTEM" => Some(Self::RoleSystem),
            "ROLE_FUNCTION" => Some(Self::RoleFunction),
            "ROLE_TOOL" => Some(Self::RoleTool),
            _ => None,
        }
    }
}
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum ReasoningEffort {
    InvalidEffort = 0,
    EffortLow = 1,
    EffortMedium = 2,
    EffortHigh = 3,
}
impl ReasoningEffort {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            ReasoningEffort::InvalidEffort => "INVALID_EFFORT",
            ReasoningEffort::EffortLow => "EFFORT_LOW",
            ReasoningEffort::EffortMedium => "EFFORT_MEDIUM",
            ReasoningEffort::EffortHigh => "EFFORT_HIGH",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "INVALID_EFFORT" => Some(Self::InvalidEffort),
            "EFFORT_LOW" => Some(Self::EffortLow),
            "EFFORT_MEDIUM" => Some(Self::EffortMedium),
            "EFFORT_HIGH" => Some(Self::EffortHigh),
            _ => None,
        }
    }
}
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum ToolMode {
    /// Invalid tool mode.
    Invalid = 0,
    /// Let the model decide if a tool shall be used.
    Auto = 1,
    /// Force the model to not use tools.
    None = 2,
    /// Force the model to use tools.
    Required = 3,
}
impl ToolMode {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            ToolMode::Invalid => "TOOL_MODE_INVALID",
            ToolMode::Auto => "TOOL_MODE_AUTO",
            ToolMode::None => "TOOL_MODE_NONE",
            ToolMode::Required => "TOOL_MODE_REQUIRED",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "TOOL_MODE_INVALID" => Some(Self::Invalid),
            "TOOL_MODE_AUTO" => Some(Self::Auto),
            "TOOL_MODE_NONE" => Some(Self::None),
            "TOOL_MODE_REQUIRED" => Some(Self::Required),
            _ => None,
        }
    }
}
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum FormatType {
    /// Invalid format type.
    Invalid = 0,
    /// Raw text.
    Text = 1,
    /// Any JSON object.
    JsonObject = 2,
    /// Follow a JSON schema.
    JsonSchema = 3,
}
impl FormatType {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            FormatType::Invalid => "FORMAT_TYPE_INVALID",
            FormatType::Text => "FORMAT_TYPE_TEXT",
            FormatType::JsonObject => "FORMAT_TYPE_JSON_OBJECT",
            FormatType::JsonSchema => "FORMAT_TYPE_JSON_SCHEMA",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "FORMAT_TYPE_INVALID" => Some(Self::Invalid),
            "FORMAT_TYPE_TEXT" => Some(Self::Text),
            "FORMAT_TYPE_JSON_OBJECT" => Some(Self::JsonObject),
            "FORMAT_TYPE_JSON_SCHEMA" => Some(Self::JsonSchema),
            _ => None,
        }
    }
}
/// Mode to control the web search.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum SearchMode {
    InvalidSearchMode = 0,
    OffSearchMode = 1,
    OnSearchMode = 2,
    AutoSearchMode = 3,
}
impl SearchMode {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            SearchMode::InvalidSearchMode => "INVALID_SEARCH_MODE",
            SearchMode::OffSearchMode => "OFF_SEARCH_MODE",
            SearchMode::OnSearchMode => "ON_SEARCH_MODE",
            SearchMode::AutoSearchMode => "AUTO_SEARCH_MODE",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "INVALID_SEARCH_MODE" => Some(Self::InvalidSearchMode),
            "OFF_SEARCH_MODE" => Some(Self::OffSearchMode),
            "ON_SEARCH_MODE" => Some(Self::OnSearchMode),
            "AUTO_SEARCH_MODE" => Some(Self::AutoSearchMode),
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod chat_client {
    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// An API that exposes our language models via a Chat interface.
    #[derive(Debug, Clone)]
    pub struct ChatClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ChatClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ChatClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ChatClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + Send + Sync,
        {
            ChatClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Samples a response from the model and blocks until the response has been
        /// fully generated.
        pub async fn get_completion(
            &mut self,
            request: impl tonic::IntoRequest<super::GetCompletionsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::GetChatCompletionResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Chat/GetCompletion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Chat", "GetCompletion"));
            self.inner.unary(req, path, codec).await
        }
        /// Samples a response from the model and streams out the model tokens as they
        /// are being generated.
        pub async fn get_completion_chunk(
            &mut self,
            request: impl tonic::IntoRequest<super::GetCompletionsRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::GetChatCompletionChunk>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Chat/GetCompletionChunk",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Chat", "GetCompletionChunk"));
            self.inner.server_streaming(req, path, codec).await
        }
        /// Starts sampling of the model and immediately returns a response containing
        /// a request id. The request id may be used to poll
        /// the `GetDeferredCompletion` RPC.
        pub async fn start_deferred_completion(
            &mut self,
            request: impl tonic::IntoRequest<super::GetCompletionsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::StartDeferredResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Chat/StartDeferredCompletion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Chat", "StartDeferredCompletion"));
            self.inner.unary(req, path, codec).await
        }
        /// Gets the result of a deferred completion started by calling `StartDeferredCompletion`.
        pub async fn get_deferred_completion(
            &mut self,
            request: impl tonic::IntoRequest<super::GetDeferredRequest>,
        ) -> std::result::Result<
            tonic::Response<super::GetDeferredCompletionResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Chat/GetDeferredCompletion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Chat", "GetDeferredCompletion"));
            self.inner.unary(req, path, codec).await
        }
        /// Retrieve a stored response using the response ID.
        pub async fn get_stored_completion(
            &mut self,
            request: impl tonic::IntoRequest<super::GetStoredCompletionRequest>,
        ) -> std::result::Result<
            tonic::Response<super::GetChatCompletionResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Chat/GetStoredCompletion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Chat", "GetStoredCompletion"));
            self.inner.unary(req, path, codec).await
        }
        /// Delete a stored response using the response ID.
        pub async fn delete_stored_completion(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteStoredCompletionRequest>,
        ) -> std::result::Result<
            tonic::Response<super::DeleteStoredCompletionResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Chat/DeleteStoredCompletion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Chat", "DeleteStoredCompletion"));
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request message for generating embeddings.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct EmbedRequest {
    /// The entities to embed. Note that not every model supports images and text.
    /// Some models are text-only and some are image-only. You can at most embed
    /// 128 inputs in a single request.
    #[prost(message, repeated, tag = "1")]
    pub input: ::prost::alloc::vec::Vec<EmbedInput>,
    /// Name or alias of the embedding model to use.
    #[prost(string, tag = "2")]
    pub model: ::prost::alloc::string::String,
    /// Format of the returned embeddings.
    #[prost(enumeration = "EmbedEncodingFormat", tag = "3")]
    pub encoding_format: i32,
    /// An opaque string supplied by the API client (customer) to identify a user.
    /// The string will be stored in the logs and can be used in customer service
    /// requests to identify certain requests.
    #[prost(string, tag = "4")]
    pub user: ::prost::alloc::string::String,
}
/// Input content to be embedded.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct EmbedInput {
    #[prost(oneof = "embed_input::Input", tags = "1, 2")]
    pub input: ::core::option::Option<embed_input::Input>,
}
/// Nested message and enum types in `EmbedInput`.
pub mod embed_input {
    #[allow(clippy::derive_partial_eq_without_eq)]
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Input {
        /// A string to be embedded.
        #[prost(string, tag = "1")]
        String(::prost::alloc::string::String),
        /// An image to be embedded.
        #[prost(message, tag = "2")]
        ImageUrl(super::ImageUrlContent),
    }
}
/// Response object for the `Embed` RPC.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct EmbedResponse {
    /// An identifier of this request. The same ID will be used in your billing
    /// records.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// The embeddings generated from the inputs.
    #[prost(message, repeated, tag = "2")]
    pub embeddings: ::prost::alloc::vec::Vec<Embedding>,
    /// The usage associated with this request.
    #[prost(message, optional, tag = "3")]
    pub usage: ::core::option::Option<EmbeddingUsage>,
    /// The name of the model used for the request. This model name contains
    /// the actual model name used rather than any aliases.
    /// This means it can be `embed-0205` even when the request was specifying
    /// `embed-latest`.
    #[prost(string, tag = "4")]
    pub model: ::prost::alloc::string::String,
    /// This fingerprint represents the backend configuration that the model runs
    /// with.
    #[prost(string, tag = "5")]
    pub system_fingerprint: ::prost::alloc::string::String,
}
/// Holds the embedding vector for a single embedding input.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct Embedding {
    /// The index of the input this embedding was produced from.
    #[prost(int32, tag = "1")]
    pub index: i32,
    /// The feature vectors derived from the inputs. Note that some inputs such as
    /// images may produce multiple feature vectors.
    #[prost(message, repeated, tag = "2")]
    pub embeddings: ::prost::alloc::vec::Vec<FeatureVector>,
}
/// A single feature vector.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct FeatureVector {
    /// The feature vector encoded as an array of floats. Only populated if
    /// the encoding format is FORMAT_FLOAT.
    #[prost(float, repeated, tag = "1")]
    pub float_array: ::prost::alloc::vec::Vec<f32>,
    /// The feature vector encoded as a base64 string. Only populated if
    /// the encoding format is FORMAT_BASE64.
    #[prost(string, tag = "2")]
    pub base64_array: ::prost::alloc::string::String,
}
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum EmbedEncodingFormat {
    /// Invalid format.
    FormatInvalid = 0,
    /// Returns the embeddings as an array of floats.
    FormatFloat = 1,
    /// Returns the embeddings as a base64-encoded string.
    FormatBase64 = 2,
}
impl EmbedEncodingFormat {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            EmbedEncodingFormat::FormatInvalid => "FORMAT_INVALID",
            EmbedEncodingFormat::FormatFloat => "FORMAT_FLOAT",
            EmbedEncodingFormat::FormatBase64 => "FORMAT_BASE64",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "FORMAT_INVALID" => Some(Self::FormatInvalid),
            "FORMAT_FLOAT" => Some(Self::FormatFloat),
            "FORMAT_BASE64" => Some(Self::FormatBase64),
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod embedder_client {
    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// An API service for interaction with available embedding models.
    #[derive(Debug, Clone)]
    pub struct EmbedderClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl EmbedderClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> EmbedderClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> EmbedderClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + Send + Sync,
        {
            EmbedderClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Produces one embedding for each input object. The size of the produced
        /// feature vectors depends on the chosen model.
        pub async fn embed(
            &mut self,
            request: impl tonic::IntoRequest<super::EmbedRequest>,
        ) -> std::result::Result<tonic::Response<super::EmbedResponse>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static("/xai_api.Embedder/Embed");
            let mut req = request.into_request();
            req.extensions_mut().insert(GrpcMethod::new("xai_api.Embedder", "Embed"));
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request to get details of a specific model by name.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct GetModelRequest {
    /// The name of the model to retrieve details about.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Describes a language model available on the platform.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct LanguageModel {
    /// The model name used in API requests/responses.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The aliases of the name, which can also be used in lieu of name in the API
    /// requests.
    #[prost(string, repeated, tag = "11")]
    pub aliases: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The version number of this model. Used to identify minor updates when
    /// the model name is not changed.
    #[prost(string, tag = "2")]
    pub version: ::prost::alloc::string::String,
    /// The supported input modalities of the model.
    #[prost(enumeration = "Modality", repeated, tag = "3")]
    pub input_modalities: ::prost::alloc::vec::Vec<i32>,
    /// The supported output modalities of the model.
    #[prost(enumeration = "Modality", repeated, tag = "4")]
    pub output_modalities: ::prost::alloc::vec::Vec<i32>,
    /// The price (in 1/100 USD cents) per one million text prompt tokens.
    #[prost(int64, tag = "5")]
    pub prompt_text_token_price: i64,
    /// The price (in 1/100 USD cents) per one million image prompt tokens.
    #[prost(int64, tag = "6")]
    pub prompt_image_token_price: i64,
    /// The price (in USD cents) per 100 million cached text prompt tokens.
    #[prost(int64, tag = "12")]
    pub cached_prompt_token_price: i64,
    /// The price (in 1/100 USD cents) per one million text completion token.
    #[prost(int64, tag = "7")]
    pub completion_text_token_price: i64,
    /// The price (in 1/100 USD cents) per one million searches.
    #[prost(int64, tag = "13")]
    pub search_price: i64,
    /// The creation time of the model.
    #[prost(message, optional, tag = "8")]
    pub created: ::core::option::Option<::prost_types::Timestamp>,
    /// Maximum length of the prompt/input (this includes tokens of all kinds).
    /// This is typically known as the context length of the model.
    #[prost(int32, tag = "9")]
    pub max_prompt_length: i32,
    /// Fingerprint of the unique configuration of the model.
    #[prost(string, tag = "10")]
    pub system_fingerprint: ::prost::alloc::string::String,
}
/// Response from ListLanguageModels including a list of language models.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ListLanguageModelsResponse {
    /// A list of language models.
    #[prost(message, repeated, tag = "1")]
    pub models: ::prost::alloc::vec::Vec<LanguageModel>,
}
/// Describes an embedding model available on the platform.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct EmbeddingModel {
    /// The name under which the model is available in the API.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The aliases of the name, which can also be used in lieu of name in the API
    /// requests.
    #[prost(string, repeated, tag = "11")]
    pub aliases: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The version number of this model. Used to identify minor updates when
    /// the model name is not changed.
    #[prost(string, tag = "2")]
    pub version: ::prost::alloc::string::String,
    /// The supported input modalities of the model.
    #[prost(enumeration = "Modality", repeated, tag = "3")]
    pub input_modalities: ::prost::alloc::vec::Vec<i32>,
    /// The supported output modalities of the model.
    #[prost(enumeration = "Modality", repeated, tag = "4")]
    pub output_modalities: ::prost::alloc::vec::Vec<i32>,
    /// The price (in 1/100 USD cents) per one million text prompt tokens.
    #[prost(int64, tag = "5")]
    pub prompt_text_token_price: i64,
    /// The price (in 1/100 USD cents) per one million image prompt tokens.
    #[prost(int64, tag = "6")]
    pub prompt_image_token_price: i64,
    /// The creation time of the model.
    #[prost(message, optional, tag = "7")]
    pub created: ::core::option::Option<::prost_types::Timestamp>,
    /// Fingerprint of the unique configuration of the model.
    #[prost(string, tag = "8")]
    pub system_fingerprint: ::prost::alloc::string::String,
}
/// Response from ListEmbeddingModels including a list of embedding models.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ListEmbeddingModelsResponse {
    /// A list of embedding model(s).
    #[prost(message, repeated, tag = "1")]
    pub models: ::prost::alloc::vec::Vec<EmbeddingModel>,
}
/// Describes a language model available on the platform.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ImageGenerationModel {
    /// The model name used in API requests/responses.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The aliases of the name, which can also be used in lieu of name in the API
    /// requests.
    #[prost(string, repeated, tag = "11")]
    pub aliases: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The version number of this model. Used to identify minor updates when
    /// the model name is not changed.
    #[prost(string, tag = "2")]
    pub version: ::prost::alloc::string::String,
    /// The supported input modalities of the model.
    #[prost(enumeration = "Modality", repeated, tag = "3")]
    pub input_modalities: ::prost::alloc::vec::Vec<i32>,
    /// The supported output modalities of the model.
    #[prost(enumeration = "Modality", repeated, tag = "6")]
    pub output_modalities: ::prost::alloc::vec::Vec<i32>,
    /// The price (in USD cents) per image.
    #[prost(int64, tag = "12")]
    pub image_price: i64,
    /// When the language model was created.
    #[prost(message, optional, tag = "8")]
    pub created: ::core::option::Option<::prost_types::Timestamp>,
    /// Maximum length of the prompt/input (this includes tokens of all kinds).
    /// This is typically known as the context length of the model.
    #[prost(int32, tag = "9")]
    pub max_prompt_length: i32,
    /// Fingerprint of the unique configuration of the model.
    #[prost(string, tag = "10")]
    pub system_fingerprint: ::prost::alloc::string::String,
}
/// Response from ListImageGenerationModels including a list of image generation
/// models.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct ListImageGenerationModelsResponse {
    /// A list of image generation models.
    #[prost(message, repeated, tag = "1")]
    pub models: ::prost::alloc::vec::Vec<ImageGenerationModel>,
}
/// Modalities supported by a model input/output.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum Modality {
    /// Invalid modality.
    InvalidModality = 0,
    /// Text input/output.
    Text = 1,
    /// Image input/output.
    Image = 2,
    /// Embedding input/output.
    Embedding = 3,
}
impl Modality {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Modality::InvalidModality => "INVALID_MODALITY",
            Modality::Text => "TEXT",
            Modality::Image => "IMAGE",
            Modality::Embedding => "EMBEDDING",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "INVALID_MODALITY" => Some(Self::InvalidModality),
            "TEXT" => Some(Self::Text),
            "IMAGE" => Some(Self::Image),
            "EMBEDDING" => Some(Self::Embedding),
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod models_client {
    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// An API service that let users get details of available models on the
    /// platform.
    #[derive(Debug, Clone)]
    pub struct ModelsClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ModelsClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ModelsClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ModelsClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + Send + Sync,
        {
            ModelsClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Lists all language models available to your team (based on the API key).
        pub async fn list_language_models(
            &mut self,
            request: impl tonic::IntoRequest<()>,
        ) -> std::result::Result<
            tonic::Response<super::ListLanguageModelsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Models/ListLanguageModels",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Models", "ListLanguageModels"));
            self.inner.unary(req, path, codec).await
        }
        /// Lists all embedding models available to your team (based on the API key).
        pub async fn list_embedding_models(
            &mut self,
            request: impl tonic::IntoRequest<()>,
        ) -> std::result::Result<
            tonic::Response<super::ListEmbeddingModelsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Models/ListEmbeddingModels",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Models", "ListEmbeddingModels"));
            self.inner.unary(req, path, codec).await
        }
        /// Lists all image generation models available to your team (based on the API key).
        pub async fn list_image_generation_models(
            &mut self,
            request: impl tonic::IntoRequest<()>,
        ) -> std::result::Result<
            tonic::Response<super::ListImageGenerationModelsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Models/ListImageGenerationModels",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Models", "ListImageGenerationModels"));
            self.inner.unary(req, path, codec).await
        }
        /// Get details of a specific language model by model name.
        pub async fn get_language_model(
            &mut self,
            request: impl tonic::IntoRequest<super::GetModelRequest>,
        ) -> std::result::Result<tonic::Response<super::LanguageModel>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Models/GetLanguageModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Models", "GetLanguageModel"));
            self.inner.unary(req, path, codec).await
        }
        /// Get details of a specific embedding model by model name.
        pub async fn get_embedding_model(
            &mut self,
            request: impl tonic::IntoRequest<super::GetModelRequest>,
        ) -> std::result::Result<tonic::Response<super::EmbeddingModel>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Models/GetEmbeddingModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Models", "GetEmbeddingModel"));
            self.inner.unary(req, path, codec).await
        }
        /// Get details of a specific image generation model by model name.
        pub async fn get_image_generation_model(
            &mut self,
            request: impl tonic::IntoRequest<super::GetModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ImageGenerationModel>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Models/GetImageGenerationModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Models", "GetImageGenerationModel"));
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request to convert text to a sequence of tokens.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct TokenizeTextRequest {
    /// Text to tokenize.
    #[prost(string, tag = "1")]
    pub text: ::prost::alloc::string::String,
    /// Name or alias of the model used for tokenization.
    #[prost(string, tag = "2")]
    pub model: ::prost::alloc::string::String,
    /// An opaque string supplied by the API client (customer) to identify a user.
    /// The string will be stored in the logs and can be used in customer service
    /// requests to identify certain requests.
    #[prost(string, tag = "3")]
    pub user: ::prost::alloc::string::String,
}
/// Information on a token.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct Token {
    /// ID of the token.
    #[prost(uint32, tag = "1")]
    pub token_id: u32,
    /// String snippet of the token.
    #[prost(string, tag = "2")]
    pub string_token: ::prost::alloc::string::String,
    /// Bytes representing the token.
    #[prost(bytes = "vec", tag = "4")]
    pub token_bytes: ::prost::alloc::vec::Vec<u8>,
}
/// Response including the tokenization result.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message, Builder)]
pub struct TokenizeTextResponse {
    /// The sequence of tokens. This is the output of the tokenization process.
    #[prost(message, repeated, tag = "1")]
    pub tokens: ::prost::alloc::vec::Vec<Token>,
    /// The name of the model used for the request. This model name contains
    /// the actual model name used rather than any aliases.
    /// This means the this can be `grok-2-1212` even when the request was
    /// specifying `grok-2-latest`.
    #[prost(string, tag = "2")]
    pub model: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod tokenize_client {
    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// An API service to tokenize input prompts.
    #[derive(Debug, Clone)]
    pub struct TokenizeClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl TokenizeClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> TokenizeClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> TokenizeClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + Send + Sync,
        {
            TokenizeClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Convert text to a sequence of tokens.
        pub async fn tokenize_text(
            &mut self,
            request: impl tonic::IntoRequest<super::TokenizeTextRequest>,
        ) -> std::result::Result<
            tonic::Response<super::TokenizeTextResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/xai_api.Tokenize/TokenizeText",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("xai_api.Tokenize", "TokenizeText"));
            self.inner.unary(req, path, codec).await
        }
    }
}
